#!/usr/bin/env python3
"""
Phase 5: Comprehensive Testing & Validation
Professional testing suite for Universal Workshop ERP refactoring validation

This comprehensive testing framework validates that all refactoring phases
completed successfully without functionality regression or data loss.
"""

import os
import sys
import json
import time
import traceback
import subprocess
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import importlib.util

# Add the current directory to sys.path for imports
if os.getcwd() not in sys.path:
    sys.path.insert(0, os.getcwd())


class Phase5TestingManager:
    """Comprehensive testing manager for Phase 5 validation"""
    
    def __init__(self):
        self.start_time = datetime.now()
        self.app_path = Path("apps/universal_workshop/universal_workshop")
        self.log_file = "phase5_testing_validation.log"
        self.report_file = "phase5_testing_report.json"
        
        # Test results tracking
        self.test_results = []
        self.test_stats = {
            "total_tests": 0,
            "passed_tests": 0,
            "failed_tests": 0,
            "warning_tests": 0,
            "skipped_tests": 0
        }
        
        # Error and warning tracking
        self.errors = []
        self.warnings = []
        self.critical_issues = []
        
        # Performance metrics
        self.performance_metrics = {}
        self.baseline_metrics = {}
        
        # Data integrity tracking
        self.data_integrity_results = {}
        
        # System functionality validation
        self.functionality_results = {}
        
    def log(self, message, level="INFO"):
        """Log message with timestamp and level"""
        timestamp = datetime.now().isoformat()
        log_entry = f"[{timestamp}] {level}: {message}"
        
        # Console output with colors
        colors = {
            "INFO": "\033[94m",      # Blue
            "SUCCESS": "\033[92m",   # Green
            "WARNING": "\033[93m",   # Yellow
            "ERROR": "\033[91m",     # Red
            "CRITICAL": "\033[95m",  # Magenta
            "RESET": "\033[0m"       # Reset
        }
        
        color = colors.get(level, colors["INFO"])
        print(f"{color}{log_entry}{colors['RESET']}")
        
        # File logging
        with open(self.log_file, "a", encoding='utf-8') as f:
            f.write(log_entry + "\n")
    
    def create_safety_checkpoint(self):
        """Create safety checkpoint before testing"""
        try:
            tag_name = f"phase5-before-testing-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
            result = subprocess.run(["git", "tag", tag_name], 
                                  capture_output=True, text=True, check=True)
            
            self.log(f"‚úÖ Safety checkpoint created: {tag_name}", "SUCCESS")
            return tag_name
        except subprocess.CalledProcessError as e:
            self.log(f"‚ùå Failed to create safety checkpoint: {e}", "ERROR")
            return None
        except Exception as e:
            self.log(f"‚ùå Unexpected error creating checkpoint: {e}", "ERROR")
            return None
    
    def run_test(self, test_name, test_function, critical=False):
        """Run a single test with comprehensive error handling"""
        self.test_stats["total_tests"] += 1
        start_time = time.time()
        
        try:
            self.log(f"üß™ Running: {test_name}")
            result = test_function()
            execution_time = time.time() - start_time
            
            if result is True:
                self.test_stats["passed_tests"] += 1
                self.log(f"‚úÖ PASSED: {test_name} ({execution_time:.2f}s)", "SUCCESS")
                status = "PASSED"
            elif result is False:
                self.test_stats["failed_tests"] += 1
                self.log(f"‚ùå FAILED: {test_name} ({execution_time:.2f}s)", "ERROR")
                status = "FAILED"
                if critical:
                    self.critical_issues.append(f"Critical test failed: {test_name}")
            elif result == "WARNING":
                self.test_stats["warning_tests"] += 1
                self.log(f"‚ö†Ô∏è WARNING: {test_name} ({execution_time:.2f}s)", "WARNING")
                status = "WARNING"
            elif result == "SKIPPED":
                self.test_stats["skipped_tests"] += 1
                self.log(f"‚è≠Ô∏è SKIPPED: {test_name} ({execution_time:.2f}s)", "INFO")
                status = "SKIPPED"
            else:
                self.test_stats["warning_tests"] += 1
                self.log(f"‚ö†Ô∏è UNCLEAR: {test_name} - {result} ({execution_time:.2f}s)", "WARNING")
                status = "UNCLEAR"
                
        except Exception as e:
            execution_time = time.time() - start_time
            self.test_stats["failed_tests"] += 1
            error_msg = f"Test {test_name} failed with exception: {str(e)}"
            self.log(f"‚ùå EXCEPTION: {error_msg} ({execution_time:.2f}s)", "ERROR")
            self.errors.append(error_msg)
            
            if critical:
                self.critical_issues.append(f"Critical test exception: {test_name} - {str(e)}")
            
            status = "EXCEPTION"
            
        # Record test result
        test_result = {
            "test_name": test_name,
            "status": status,
            "execution_time": execution_time,
            "timestamp": datetime.now().isoformat(),
            "critical": critical
        }
        self.test_results.append(test_result)
        
        return status in ["PASSED", "WARNING", "SKIPPED"]
    
    # ==========================================
    # COMPREHENSIVE SYSTEM TESTING
    # ==========================================
    
    def test_directory_structure_integrity(self):
        """Test that new directory structure is complete and accessible"""
        try:
            required_directories = [
                "core/boot",
                "core/permissions", 
                "core/session",
                "core/monitoring",
                "setup/installation",
                "setup/onboarding",
                "setup/licensing", 
                "setup/branding",
                "workshop_operations/profiles",
                "workshop_operations/service_management",
                "workshop_operations/technician_management",
                "workshop_operations/quality_control",
                "system_administration/backup_management",
                "system_administration/performance_monitoring",
                "system_administration/error_handling",
                "system_administration/integration_management",
                "mobile_operations/device_management",
                "mobile_operations/offline_capabilities",
                "mobile_operations/pwa_components",
                "assets/js/core",
                "assets/js/modules",
                "assets/js/shared",
                "assets/css/core",
                "assets/css/themes",
                "assets/css/modules"
            ]
            
            missing_dirs = []
            for directory in required_directories:
                dir_path = self.app_path / directory
                if not dir_path.exists():
                    missing_dirs.append(directory)
            
            if missing_dirs:
                self.warnings.append(f"Missing directories: {', '.join(missing_dirs)}")
                return "WARNING"
            
            return True
            
        except Exception as e:
            self.errors.append(f"Directory structure test failed: {e}")
            return False
    
    def test_python_import_integrity(self):
        """Test all Python imports work correctly after refactoring"""
        try:
            # Test core imports by checking if files exist and are syntactically valid
            core_imports = [
                ("universal_workshop.core.boot.boot_manager", "apps/universal_workshop/universal_workshop/core/boot/boot_manager.py"),
                ("universal_workshop.setup.installation.installation_manager", "apps/universal_workshop/universal_workshop/setup/installation/installation_manager.py")
            ]
            
            failed_imports = []
            for import_path, file_path in core_imports:
                # Check if file exists
                if not os.path.exists(file_path):
                    failed_imports.append(f"{import_path} - file not found: {file_path}")
                    continue
                
                # Check if file is syntactically valid Python
                try:
                    with open(file_path, 'r') as f:
                        source = f.read()
                    compile(source, file_path, 'exec')
                except SyntaxError as e:
                    failed_imports.append(f"{import_path} - syntax error: {str(e)}")
                except Exception as e:
                    failed_imports.append(f"{import_path} - compile error: {str(e)}")
            
            if failed_imports:
                self.errors.extend(failed_imports)
                return False
                
            return True
            
        except Exception as e:
            self.errors.append(f"Import integrity test failed: {e}")
            return False
    
    def test_database_connectivity(self):
        """Test database connectivity and basic operations"""
        try:
            # This would require frappe context, so we'll do basic checks
            if not os.path.exists("sites"):
                return "SKIPPED"  # Not in Frappe environment
                
            # Check if we can access the database configuration
            return True
            
        except Exception as e:
            self.errors.append(f"Database connectivity test failed: {e}")
            return False
    
    def test_frontend_asset_accessibility(self):
        """Test that all frontend assets are accessible after reorganization"""
        try:
            js_dirs = ["core", "setup", "branding", "workshop", "mobile", "shared", "analytics", "modules"]
            css_dirs = ["core", "themes", "localization", "branding", "workshop", "mobile", "modules"]
            
            missing_assets = []
            
            # Check JavaScript directories
            for js_dir in js_dirs:
                js_path = self.app_path / "assets" / "js" / js_dir
                if js_path.exists():
                    js_files = list(js_path.glob("*.js"))
                    if not js_files:
                        missing_assets.append(f"No JS files in {js_dir}")
            
            # Check CSS directories  
            for css_dir in css_dirs:
                css_path = self.app_path / "assets" / "css" / css_dir
                if css_path.exists():
                    css_files = list(css_path.glob("*.css"))
                    if not css_files:
                        missing_assets.append(f"No CSS files in {css_dir}")
            
            if missing_assets:
                self.warnings.extend(missing_assets)
                return "WARNING"
                
            return True
            
        except Exception as e:
            self.errors.append(f"Frontend asset test failed: {e}")
            return False
    
    def test_hooks_file_integrity(self):
        """Test hooks.py file integrity and organization"""
        try:
            hooks_file = self.app_path / "hooks.py"
            if not hooks_file.exists():
                self.errors.append("hooks.py file not found")
                return False
                
            with open(hooks_file, 'r') as f:
                content = f.read()
            
            # Check for organized asset structure
            required_patterns = [
                "app_include_js",
                "app_include_css", 
                "/assets/universal_workshop/"
            ]
            
            missing_patterns = []
            for pattern in required_patterns:
                if pattern not in content:
                    missing_patterns.append(pattern)
            
            if missing_patterns:
                self.warnings.append(f"Missing patterns in hooks.py: {', '.join(missing_patterns)}")
                return "WARNING"
                
            return True
            
        except Exception as e:
            self.errors.append(f"Hooks file test failed: {e}")
            return False
    
    def test_doctype_accessibility(self):
        """Test that all DocTypes are accessible in their new locations"""
        try:
            # Check if DocType JSON files exist in new structure
            doctype_categories = {
                "workshop_operations": ["workshop_profile", "service_order", "technician"],
                "system_administration": ["backup_manager", "performance_monitor"],
                "mobile_operations": ["mobile_device_management"],
                "setup": ["onboarding_progress"]
            }
            
            missing_doctypes = []
            for category, doctypes in doctype_categories.items():
                category_path = self.app_path / category
                if category_path.exists():
                    for doctype in doctypes:
                        # Look for doctype directories
                        possible_paths = [
                            category_path / "doctype" / doctype,
                            category_path / f"{doctype.replace('_', '')}" / "doctype" / doctype,
                        ]
                        
                        found = False
                        for path in possible_paths:
                            if path.exists():
                                found = True
                                break
                        
                        if not found:
                            missing_doctypes.append(f"{category}/{doctype}")
            
            if missing_doctypes:
                self.warnings.append(f"DocTypes not found in expected locations: {', '.join(missing_doctypes)}")
                return "WARNING"
                
            return True
            
        except Exception as e:
            self.errors.append(f"DocType accessibility test failed: {e}")
            return False
    
    def test_build_process(self):
        """Test that the build process works correctly"""
        try:
            # Try to run bench build for the app
            result = subprocess.run(
                ["bench", "build", "--app", "universal_workshop"],
                capture_output=True, text=True, timeout=300
            )
            
            if result.returncode == 0:
                return True
            else:
                self.errors.append(f"Build failed: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            self.errors.append("Build process timed out")
            return False
        except FileNotFoundError:
            return "SKIPPED"  # bench command not available
        except Exception as e:
            self.errors.append(f"Build test failed: {e}")
            return False
    
    # ==========================================
    # PERFORMANCE TESTING
    # ==========================================
    
    def measure_import_performance(self):
        """Measure import performance for key modules"""
        try:
            import_tests = [
                "universal_workshop.core.boot.boot_manager",
                "universal_workshop.setup.installation.installation_manager"
            ]
            
            import_times = {}
            for import_path in import_tests:
                start_time = time.time()
                try:
                    spec = importlib.util.find_spec(import_path)
                    if spec:
                        import_time = time.time() - start_time
                        import_times[import_path] = import_time
                except Exception:
                    import_times[import_path] = None
            
            self.performance_metrics["import_times"] = import_times
            
            # Check if any imports are unusually slow (>1 second)
            slow_imports = [path for path, time in import_times.items() 
                          if time and time > 1.0]
            
            if slow_imports:
                self.warnings.append(f"Slow imports detected: {', '.join(slow_imports)}")
                return "WARNING"
                
            return True
            
        except Exception as e:
            self.errors.append(f"Import performance test failed: {e}")
            return False
    
    def measure_file_access_performance(self):
        """Measure file access performance for new structure"""
        try:
            access_times = {}
            
            # Test directory access times
            test_paths = [
                self.app_path / "core",
                self.app_path / "setup", 
                self.app_path / "workshop_operations",
                self.app_path / "assets" / "js",
                self.app_path / "assets" / "css"
            ]
            
            for path in test_paths:
                if path.exists():
                    start_time = time.time()
                    list(path.iterdir())  # List directory contents
                    access_time = time.time() - start_time
                    access_times[str(path.relative_to(self.app_path))] = access_time
            
            self.performance_metrics["file_access_times"] = access_times
            
            # Check for unusually slow access (>0.1 seconds)
            slow_access = [path for path, time in access_times.items() 
                          if time > 0.1]
            
            if slow_access:
                self.warnings.append(f"Slow file access: {', '.join(slow_access)}")
                return "WARNING"
                
            return True
            
        except Exception as e:
            self.errors.append(f"File access performance test failed: {e}")
            return False
    
    # ==========================================
    # DATA INTEGRITY VERIFICATION
    # ==========================================
    
    def verify_no_duplicate_files(self):
        """Verify no duplicate files exist after refactoring"""
        try:
            file_hashes = defaultdict(list)
            
            # Scan for potential duplicates
            for file_path in self.app_path.rglob("*.py"):
                if file_path.is_file():
                    try:
                        # Simple size-based check (could be enhanced with actual hashing)
                        file_size = file_path.stat().st_size
                        file_hashes[file_size].append(str(file_path.relative_to(self.app_path)))
                    except Exception:
                        continue
            
            # Find potential duplicates (same size)
            potential_duplicates = []
            for size, files in file_hashes.items():
                if len(files) > 1:
                    potential_duplicates.extend(files)
            
            if potential_duplicates:
                self.warnings.append(f"Potential duplicate files found: {len(potential_duplicates)} files")
                self.data_integrity_results["potential_duplicates"] = potential_duplicates
                return "WARNING"
                
            self.data_integrity_results["duplicate_check"] = "CLEAN"
            return True
            
        except Exception as e:
            self.errors.append(f"Duplicate file check failed: {e}")
            return False
    
    def verify_migration_completeness(self):
        """Verify all expected files were migrated correctly"""
        try:
            # Check migration reports exist
            migration_reports = [
                "phase4_asset_migration_report.json",
                "phase4_hooks_update_report.json"
            ]
            
            missing_reports = []
            migration_data = {}
            
            for report in migration_reports:
                if os.path.exists(report):
                    try:
                        with open(report, 'r') as f:
                            migration_data[report] = json.load(f)
                    except Exception as e:
                        self.warnings.append(f"Could not read {report}: {e}")
                else:
                    missing_reports.append(report)
            
            if missing_reports:
                self.warnings.append(f"Missing migration reports: {', '.join(missing_reports)}")
                return "WARNING"
            
            # Analyze migration data if available
            if "phase4_asset_migration_report.json" in migration_data:
                report_data = migration_data["phase4_asset_migration_report.json"]
                migrated_count = report_data.get("total_files_migrated", 0)
                error_count = len(report_data.get("errors", []))
                
                self.data_integrity_results["migration_summary"] = {
                    "files_migrated": migrated_count,
                    "migration_errors": error_count
                }
                
                if error_count > 0:
                    self.warnings.append(f"Migration completed with {error_count} errors")
                    return "WARNING"
            
            return True
            
        except Exception as e:
            self.errors.append(f"Migration completeness check failed: {e}")
            return False
    
    # ==========================================
    # USER WORKFLOW TESTING
    # ==========================================
    
    def test_critical_user_workflows(self):
        """Test critical user workflows still function"""
        try:
            # This would require actual Frappe context for full testing
            # For now, we'll do structural checks
            
            workflow_components = {
                "setup": ["installation", "onboarding"],
                "workshop_operations": ["profiles", "service_management"],
                "system_administration": ["backup_management", "performance_monitoring"]
            }
            
            missing_components = []
            for category, components in workflow_components.items():
                category_path = self.app_path / category
                if category_path.exists():
                    for component in components:
                        component_path = category_path / component
                        if not component_path.exists():
                            missing_components.append(f"{category}/{component}")
            
            if missing_components:
                self.warnings.append(f"Missing workflow components: {', '.join(missing_components)}")
                return "WARNING"
                
            self.functionality_results["workflow_structure"] = "COMPLETE"
            return True
            
        except Exception as e:
            self.errors.append(f"Workflow testing failed: {e}")
            return False
    
    def test_api_endpoints_structure(self):
        """Test API endpoint structure integrity"""
        try:
            # Look for API files in new structure
            api_files = []
            for module_path in self.app_path.rglob("api.py"):
                api_files.append(str(module_path.relative_to(self.app_path)))
            
            self.functionality_results["api_files"] = api_files
            
            if not api_files:
                self.warnings.append("No API files found in new structure")
                return "WARNING"
                
            return True
            
        except Exception as e:
            self.errors.append(f"API endpoint test failed: {e}")
            return False
    
    # ==========================================
    # MAIN EXECUTION METHODS
    # ==========================================
    
    def run_comprehensive_testing(self):
        """Run all comprehensive tests"""
        self.log("üöÄ Starting Phase 5: Comprehensive Testing & Validation", "INFO")
        self.log("=" * 80, "INFO")
        
        # Create safety checkpoint
        checkpoint = self.create_safety_checkpoint()
        if not checkpoint:
            self.log("‚ö†Ô∏è Could not create safety checkpoint, proceeding anyway", "WARNING")
        
        # Define test suite
        test_suite = [
            # CRITICAL TESTS - System must pass these
            ("Directory Structure Integrity", self.test_directory_structure_integrity, True),
            ("Python Import Integrity", self.test_python_import_integrity, True),
            ("Hooks File Integrity", self.test_hooks_file_integrity, True),
            
            # IMPORTANT TESTS - Should pass, warnings acceptable
            ("Frontend Asset Accessibility", self.test_frontend_asset_accessibility, False),
            ("DocType Accessibility", self.test_doctype_accessibility, False),
            ("Database Connectivity", self.test_database_connectivity, False),
            
            # PERFORMANCE TESTS
            ("Import Performance", self.measure_import_performance, False),
            ("File Access Performance", self.measure_file_access_performance, False),
            
            # DATA INTEGRITY TESTS
            ("No Duplicate Files", self.verify_no_duplicate_files, False),
            ("Migration Completeness", self.verify_migration_completeness, False),
            
            # FUNCTIONALITY TESTS
            ("Critical User Workflows", self.test_critical_user_workflows, False),
            ("API Endpoints Structure", self.test_api_endpoints_structure, False),
            
            # BUILD TESTS
            ("Build Process", self.test_build_process, False),
        ]
        
        # Execute test suite
        self.log(f"\nüß™ Executing {len(test_suite)} comprehensive tests...", "INFO")
        
        for test_name, test_function, critical in test_suite:
            success = self.run_test(test_name, test_function, critical)
            
            # Stop on critical failures
            if not success and critical:
                self.log(f"üõë Critical test failed: {test_name}", "CRITICAL")
                self.log("Testing stopped due to critical failure", "CRITICAL")
                return False
        
        return True
    
    def generate_comprehensive_report(self):
        """Generate comprehensive testing report"""
        try:
            end_time = datetime.now()
            total_duration = (end_time - self.start_time).total_seconds()
            
            # Calculate success rate
            total_tests = self.test_stats["total_tests"]
            passed_tests = self.test_stats["passed_tests"] + self.test_stats["warning_tests"]
            success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
            
            # Determine overall status
            if self.critical_issues:
                overall_status = "CRITICAL_FAILURE"
            elif self.test_stats["failed_tests"] > 0:
                overall_status = "FAILURE"
            elif self.test_stats["warning_tests"] > 0:
                overall_status = "SUCCESS_WITH_WARNINGS"
            else:
                overall_status = "SUCCESS"
            
            # Create comprehensive report
            report = {
                "phase": "Phase 5: Testing & Validation",
                "timestamp": end_time.isoformat(),
                "duration_seconds": total_duration,
                "overall_status": overall_status,
                "success_rate_percentage": round(success_rate, 2),
                
                "test_statistics": self.test_stats,
                "test_results": self.test_results,
                
                "performance_metrics": self.performance_metrics,
                "data_integrity_results": self.data_integrity_results,
                "functionality_results": self.functionality_results,
                
                "errors": self.errors,
                "warnings": self.warnings,
                "critical_issues": self.critical_issues,
                
                "recommendations": self._generate_recommendations(),
                "next_steps": self._generate_next_steps(overall_status)
            }
            
            # Save report
            with open(self.report_file, 'w') as f:
                json.dump(report, f, indent=2)
            
            return report
            
        except Exception as e:
            self.log(f"‚ùå Failed to generate report: {e}", "ERROR")
            return None
    
    def _generate_recommendations(self):
        """Generate recommendations based on test results"""
        recommendations = []
        
        if self.warnings:
            recommendations.append("Address warning issues before proceeding to Phase 6")
        
        if self.critical_issues:
            recommendations.append("CRITICAL: Resolve critical issues before any further changes")
        
        if self.test_stats["failed_tests"] > 0:
            recommendations.append("Investigate and fix failed tests")
        
        if not recommendations:
            recommendations.append("System validation successful - ready for Phase 6")
        
        return recommendations
    
    def _generate_next_steps(self, overall_status):
        """Generate next steps based on overall status"""
        if overall_status == "CRITICAL_FAILURE":
            return [
                "STOP all refactoring activities",
                "Restore from latest safety checkpoint", 
                "Investigate critical issues",
                "Fix issues before retrying Phase 5"
            ]
        elif overall_status == "FAILURE":
            return [
                "Fix failed tests before proceeding",
                "Re-run Phase 5 after fixes",
                "Consider partial rollback if needed"
            ]
        elif overall_status == "SUCCESS_WITH_WARNINGS":
            return [
                "Review warnings and assess impact",
                "Decide whether to address warnings now or in Phase 6",
                "Proceed to Phase 6 with caution"
            ]
        else:
            return [
                "Phase 5 validation complete",
                "System ready for Phase 6: Cleanup & Optimization",
                "Proceed with confidence"
            ]
    
    def display_final_summary(self, report):
        """Display final summary of Phase 5 testing"""
        self.log("\n" + "=" * 80, "INFO")
        self.log("üéØ PHASE 5 TESTING & VALIDATION COMPLETE", "INFO")
        self.log("=" * 80, "INFO")
        
        # Status display
        status_colors = {
            "SUCCESS": "SUCCESS",
            "SUCCESS_WITH_WARNINGS": "WARNING", 
            "FAILURE": "ERROR",
            "CRITICAL_FAILURE": "CRITICAL"
        }
        
        overall_status = report["overall_status"]
        status_color = status_colors.get(overall_status, "INFO")
        
        self.log(f"üìä Overall Status: {overall_status}", status_color)
        self.log(f"‚è±Ô∏è Total Duration: {report['duration_seconds']:.2f} seconds", "INFO")
        self.log(f"‚úÖ Success Rate: {report['success_rate_percentage']}%", "INFO")
        
        # Test statistics
        stats = report["test_statistics"]
        self.log(f"\nüìà Test Statistics:", "INFO")
        self.log(f"   Total Tests: {stats['total_tests']}", "INFO")
        self.log(f"   Passed: {stats['passed_tests']}", "SUCCESS")
        self.log(f"   Failed: {stats['failed_tests']}", "ERROR" if stats['failed_tests'] > 0 else "INFO")
        self.log(f"   Warnings: {stats['warning_tests']}", "WARNING" if stats['warning_tests'] > 0 else "INFO")
        self.log(f"   Skipped: {stats['skipped_tests']}", "INFO")
        
        # Issues summary
        if report["critical_issues"]:
            self.log(f"\nüö® Critical Issues: {len(report['critical_issues'])}", "CRITICAL")
            for issue in report["critical_issues"]:
                self.log(f"   - {issue}", "CRITICAL")
        
        if report["errors"]:
            self.log(f"\n‚ùå Errors: {len(report['errors'])}", "ERROR")
            for error in report["errors"][:5]:  # Show first 5
                self.log(f"   - {error}", "ERROR")
            if len(report["errors"]) > 5:
                self.log(f"   ... and {len(report['errors']) - 5} more errors", "ERROR")
        
        if report["warnings"]:
            self.log(f"\n‚ö†Ô∏è Warnings: {len(report['warnings'])}", "WARNING")
            for warning in report["warnings"][:5]:  # Show first 5
                self.log(f"   - {warning}", "WARNING")
            if len(report["warnings"]) > 5:
                self.log(f"   ... and {len(report['warnings']) - 5} more warnings", "WARNING")
        
        # Recommendations
        self.log(f"\nüí° Recommendations:", "INFO")
        for rec in report["recommendations"]:
            self.log(f"   - {rec}", "INFO")
        
        # Next steps
        self.log(f"\nüöÄ Next Steps:", "INFO")
        for step in report["next_steps"]:
            self.log(f"   - {step}", "INFO")
        
        # Report location
        self.log(f"\nüìÅ Detailed report saved to: {self.report_file}", "INFO")
        self.log(f"üìÅ Test log saved to: {self.log_file}", "INFO")
        
        return overall_status
    
    def execute_phase5_testing(self):
        """Execute complete Phase 5 testing"""
        try:
            # Run comprehensive testing
            testing_success = self.run_comprehensive_testing()
            
            # Generate report
            report = self.generate_comprehensive_report()
            
            if not report:
                self.log("‚ùå Failed to generate testing report", "ERROR")
                return False
            
            # Display summary
            overall_status = self.display_final_summary(report)
            
            # Create completion checkpoint
            completion_tag = f"phase5-testing-complete-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
            try:
                subprocess.run(["git", "tag", completion_tag], check=True)
                self.log(f"‚úÖ Testing completion checkpoint: {completion_tag}", "SUCCESS")
            except Exception as e:
                self.log(f"‚ö†Ô∏è Could not create completion tag: {e}", "WARNING")
            
            # Return success based on overall status
            return overall_status in ["SUCCESS", "SUCCESS_WITH_WARNINGS"]
            
        except Exception as e:
            self.log(f"‚ùå Phase 5 testing failed with exception: {e}", "CRITICAL")
            self.log(f"Exception details: {traceback.format_exc()}", "CRITICAL")
            return False


def main():
    """Main Phase 5 execution"""
    manager = Phase5TestingManager()
    
    try:
        success = manager.execute_phase5_testing()
        
        if success:
            print("\nüéâ Phase 5 Testing & Validation completed successfully!")
            print("‚úÖ System is validated and ready for Phase 6: Cleanup & Optimization")
        else:
            print("\n‚ö†Ô∏è Phase 5 Testing completed with issues")
            print("‚ùå Review the test report before proceeding to Phase 6")
        
        return success
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Phase 5 testing interrupted by user")
        return False
    except Exception as e:
        print(f"\n‚ùå Phase 5 testing failed with unexpected error: {e}")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
